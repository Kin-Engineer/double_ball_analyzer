# [file name]: crawler.py
# [file content begin]
"""
åŒè‰²çƒæ•°æ®é‡‡é›†ç³»ç»Ÿ - å¢å¼ºç‰ˆ
æ·»åŠ å¤šç§æ•°æ®è·å–é€‰é¡¹ï¼Œä¿æŒåŸæœ‰500.comæ•°æ®æº
"""

import requests
import time
import random
import re
import os
import json
import logging
from datetime import datetime, timedelta
from typing import Optional, Dict, Any, List
from bs4 import BeautifulSoup

# å¯¼å…¥åŸæœ‰ç³»ç»Ÿçš„æ¨¡å—
try:
    from data.models import DoubleBallRecord
    from data.database import DoubleBallDatabase
    from config import config
    logger = logging.getLogger('crawler')
except ImportError as e:
    logging.error(f"å¯¼å…¥æ¨¡å—å¤±è´¥: {e}")
    raise

class DoubleBallCrawler:
    """åŒè‰²çƒæ•°æ®çˆ¬è™« - å¢å¼ºç‰ˆ"""
    
    def __init__(self, database: Optional[DoubleBallDatabase] = None):
        if database is None:
            # ä½¿ç”¨ config ä¸­çš„æ•°æ®åº“è·¯å¾„åˆ›å»ºæ•°æ®åº“å®ä¾‹
            try:
                from config import config
                db_path = config.paths.DATABASE_PATH
            except ImportError:
                db_path = "double_ball.db"
            database = DoubleBallDatabase(db_path)

        self.db = database
        self.base_url = "https://www.500.com/kaijiang/ssq"

        # é…ç½® - è¿™é‡Œå¯èƒ½æœ‰é—®é¢˜ï¼Œconfig.crawler å¯èƒ½æœªå®šä¹‰
        try:
            from config import config
            self.year_issues = config.crawler.YEAR_ISSUES
            self.request_timeout = config.crawler.REQUEST_TIMEOUT
            self.max_retries = config.crawler.MAX_RETRIES
            self.request_delay = config.crawler.REQUEST_DELAY
        except (ImportError, AttributeError):
            # æä¾›é»˜è®¤å€¼
            self.year_issues = {2024: 151, 2025: 151}
            self.request_timeout = 30
            self.max_retries = 3
            self.request_delay = (1.0, 3.0)
        
        # å¤šä¸ªUser-Agentè½®æ¢
        self.user_agents = [
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/121.0',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.6045.160 Safari/537.36',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36 Edg/120.0.0.0'
        ]
        
        # è¯·æ±‚ç»Ÿè®¡
        self.request_count = 0
        self.last_request_time = time.time()
    
    def smart_request(self, url: str, max_retries: int = 3) -> Optional[requests.Response]:
        """æ™ºèƒ½è¯·æ±‚å‡½æ•°ï¼Œæ·»åŠ å»¶è¿Ÿå’Œé‡è¯•æœºåˆ¶"""
        for retry in range(max_retries):
            try:
                # æ§åˆ¶è¯·æ±‚é¢‘ç‡
                current_time = time.time()
                time_since_last = current_time - self.last_request_time
                
                # åŠ¨æ€å»¶è¿Ÿ
                delay = random.uniform(self.request_delay[0], self.request_delay[1])
                if time_since_last < delay:
                    time.sleep(delay - time_since_last)
                
                # éšæœºé€‰æ‹©User-Agent
                user_agent = random.choice(self.user_agents)
                headers = {
                    'User-Agent': user_agent,
                    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                    'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
                    'Accept-Encoding': 'gzip, deflate, br',
                    'Connection': 'keep-alive',
                }
                
                response = requests.get(url, headers=headers, timeout=self.request_timeout)
                response.encoding = 'utf-8'
                
                # æ›´æ–°è¯·æ±‚ç»Ÿè®¡
                self.request_count += 1
                self.last_request_time = time.time()
                
                if response.status_code == 200:
                    return response
                elif response.status_code == 429:  # è¯·æ±‚è¿‡å¤š
                    wait_time = (retry + 1) * 10
                    logger.warning(f"è¯·æ±‚è¿‡å¤šï¼Œç­‰å¾… {wait_time} ç§’...")
                    time.sleep(wait_time)
                else:
                    logger.warning(f"HTTP {response.status_code}: {url}")
                    time.sleep((retry + 1) * 2)
                    
            except requests.exceptions.RequestException as e:
                logger.error(f"è¯·æ±‚å¼‚å¸¸: {e}")
                time.sleep((retry + 1) * 5)
        
        logger.error(f"è¯·æ±‚å¤±è´¥ï¼Œå·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°: {url}")
        return None

    def get_latest_issue_info(self) -> Dict[str, Any]:
        """è·å–æœ€æ–°æœŸå·ä¿¡æ¯"""
        try:
            # å°è¯•ä»æ•°æ®åº“è·å–æœ€æ–°è®°å½•
            records = self.db.get_all_records(order_by="CAST(issue AS INTEGER) DESC", limit=1)
            db_latest_issue = None
            if records:
                db_latest_issue = records[0].issue

            # ä»ç½‘é¡µè·å–æœ€æ–°æœŸå·
            web_latest_issue = self._get_latest_period_from_web()

            # é€‰æ‹©è¾ƒå¤§çš„æœŸå·
            if db_latest_issue and web_latest_issue:
                # å°†æœŸå·è½¬æ¢ä¸ºæ•´æ•°è¿›è¡Œæ¯”è¾ƒï¼ˆæ³¨æ„ï¼šæœŸå·æ˜¯å­—ç¬¦ä¸²ï¼Œå¦‚'26015'ï¼‰
                db_num = int(db_latest_issue)
                web_num = int(web_latest_issue)
                latest_issue = str(max(db_num, web_num))
                source = 'web' if web_num >= db_num else 'database'
            elif db_latest_issue:
                latest_issue = db_latest_issue
                source = 'database'
            elif web_latest_issue:
                latest_issue = web_latest_issue
                source = 'web'
            else:
                latest_issue = '26017'# é»˜è®¤å€¼
                source = 'fallback'

            # è·å–æœ€æ–°æœŸå·çš„æ—¥æœŸï¼ˆå¦‚æœæ˜¯æ•°æ®åº“ä¸­çš„ï¼Œåˆ™ä»æ•°æ®åº“è·å–ï¼›å¦åˆ™ç”¨ä»Šå¤©ï¼‰
            if source == 'database' and records:
                draw_date = records[0].draw_date
            else:
                draw_date = datetime.now().strftime('%Y-%m-%d')

            return {
                'issue': latest_issue,
                'date': draw_date,
                'source': source
            }

        except Exception as e:
            logger.error(f"è·å–æœ€æ–°æœŸå·å¤±è´¥: {e}")
            return {
                'issue': '26017',  # ä¿®æ”¹ä¸º26017
                'date': datetime.now().strftime('%Y-%m-%d'),
                'source': 'fallback'
            }

    def _get_latest_period_from_web(self) -> str:
        """ä»ç½‘ç«™è·å–æœ€æ–°æœŸå·"""
        try:
            # ç”Ÿæˆæµ‹è¯•æœŸå·åˆ—è¡¨ - ä»26020å¼€å§‹å€’åºæµ‹è¯•
            test_periods = []
            for i in range(20, 0, -1):
                test_periods.append(f"26{i:03d}")
            
            for period in test_periods:
                url = f"{self.base_url}/{period}.html"
                response = self.smart_request(url)
                
                if response and response.status_code == 200:
                    soup = BeautifulSoup(response.text, 'html.parser')
                    
                    # æ£€æŸ¥æ˜¯å¦æœ‰å¼€å¥–æ•°æ®
                    red_balls = []
                    red_selectors = ['.ball-red-normal.ball', '.ball_red', '[class*="red"][class*="ball"]']
                    
                    for selector in red_selectors:
                        elements = soup.select(selector)
                        for elem in elements[:6]:
                            ball_text = elem.get_text().strip()
                            if ball_text.isdigit() and 1 <= int(ball_text) <= 33:
                                red_balls.append(int(ball_text))
                    
                    if len(red_balls) >= 6:
                        logger.info(f"æ‰¾åˆ°æœ‰æ•ˆæœ€æ–°æœŸå·: {period}")
                        return period
            
            # å¦‚æœæ²¡æ‰¾åˆ°ï¼Œè¿”å›é»˜è®¤å€¼ï¼ˆæ”¹ä¸º26017ï¼‰
            # æ³¨æ„ï¼šè¿™é‡Œéœ€è¦æ ¹æ®å®é™…æƒ…å†µæ‰‹åŠ¨æ›´æ–°æœ€æ–°æœŸå·
            # # å½“å‰æœ€æ–°æœŸå·ï¼š26017 (æ›´æ–°æ—¶é—´ï¼š2026-02-06)
            # # ä¸‹æœŸå¼€å¥–åéœ€æ›´æ–°ä¸ºï¼š26017
            return "26017"
            
        except Exception as e:
            logger.error(f"ä»ç½‘ç«™è·å–æœ€æ–°æœŸå·å¤±è´¥: {e}")
            return "26017"
    
    def crawl_single_period(self, issue: str) -> Optional[DoubleBallRecord]:
        """çˆ¬å–å•æœŸæ•°æ®"""
        url = f"{self.base_url}/{issue}.html"
        
        try:
            response = self.smart_request(url)
            
            if not response or response.status_code != 200:
                logger.warning(f"è¯·æ±‚å¤±è´¥: {issue}")
                return None
            
            return self._parse_draw_page(response.text, issue)
                
        except Exception as e:
            logger.error(f"çˆ¬å–å¤±è´¥ {issue}: {e}")
            return None
    
    def _parse_draw_page(self, html: str, issue: str) -> Optional[DoubleBallRecord]:
        """è§£æå¼€å¥–é¡µé¢"""
        soup = BeautifulSoup(html, 'html.parser')
        
        try:
            # 1. æå–çº¢çƒ
            red_balls = []
            
            red_selectors = [
                '.ball-red-normal.ball',
                '.ball_red',
                'span.ball-red-normal.ball',
                'li.ball-red-normal.ball',
                '[class*="red"][class*="ball"]'
            ]
            
            for selector in red_selectors:
                red_elements = soup.select(selector)
                for elem in red_elements[:6]:
                    ball_text = elem.get_text().strip()
                    if ball_text.isdigit() and 1 <= int(ball_text) <= 33:
                        red_balls.append(int(ball_text))
                if len(red_balls) >= 6:
                    break
            
            if len(red_balls) != 6:
                logger.warning(f"çº¢çƒè§£æå¤±è´¥: {issue}, æ‰¾åˆ°{len(red_balls)}ä¸ªçƒ")
                return None
            
            # ç¡®ä¿çº¢çƒæ’åºï¼ˆDoubleBallRecord æœŸæœ›æ’åºçš„ï¼‰
            red_balls.sort()
            
            # 2. æå–è“çƒ
            blue_ball = None
            
            blue_selectors = [
                '.ball-blue-normal.ball',
                '.ball_blue',
                'span.ball-blue-normal.ball',
                'li.ball-blue-normal.ball',
                '[class*="blue"][class*="ball"]'
            ]
            
            for selector in blue_selectors:
                blue_elements = soup.select(selector)
                for elem in blue_elements:
                    ball_text = elem.get_text().strip()
                    if ball_text.isdigit() and 1 <= int(ball_text) <= 16:
                        blue_ball = int(ball_text)
                        break
                if blue_ball:
                    break
            
            if not blue_ball:
                logger.warning(f"è“çƒè§£æå¤±è´¥: {issue}")
                return None
            
            # 3. æå–æ—¥æœŸ
            draw_date = self._extract_date_from_page(soup, issue)
            
            # 4. åˆ›å»º DoubleBallRecord å¯¹è±¡
            record = DoubleBallRecord(
                issue=issue,
                draw_date=draw_date,
                red1=red_balls[0],
                red2=red_balls[1],
                red3=red_balls[2],
                red4=red_balls[3],
                red5=red_balls[4],
                red6=red_balls[5],
                blue=blue_ball
            )
            
            # 5. è®¡ç®—ç‰¹å¾
            record.calculate_basic_features()
            record.calculate_stage1_features()
            
            logger.info(f"æˆåŠŸè§£æ: {issue}, çº¢çƒ{red_balls}, è“çƒ{blue_ball}, æ—¥æœŸ{draw_date}")
            return record
            
        except Exception as e:
            logger.error(f"è§£æé¡µé¢å¤±è´¥ {issue}: {e}")
            return None
    
    def _extract_date_from_page(self, soup: BeautifulSoup, issue: str) -> str:
        """ä»é¡µé¢æå–æ—¥æœŸ"""
        page_text = soup.get_text()
        
        date_patterns = [
            r'å¼€å¥–æ—¥æœŸ[ï¼š:\s]*(\d{4}å¹´\d{1,2}æœˆ\d{1,2}æ—¥)',
            r'å¼€å¥–æ—¥æœŸ[ï¼š:\s]*(\d{4}-\d{1,2}-\d{1,2})',
            r'(\d{4}å¹´\d{1,2}æœˆ\d{1,2}æ—¥)',
            r'(\d{4}-\d{1,2}-\d{1,2})'
        ]
        
        for pattern in date_patterns:
            matches = re.findall(pattern, page_text)
            for match in matches:
                if '2023-2-13' not in match and '2023å¹´2æœˆ13æ—¥' not in match:
                    parsed_date = self._parse_date_string(match)
                    if parsed_date:
                        return parsed_date
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ—¥æœŸï¼Œä»æœŸå·ä¼°ç®—
        return self._estimate_date_from_issue(issue)
    
    def _parse_date_string(self, date_str: str) -> Optional[str]:
        """è§£ææ—¥æœŸå­—ç¬¦ä¸²"""
        try:
            date_str = date_str.strip()
            
            # ä¸­æ–‡æ—¥æœŸæ ¼å¼
            chinese_match = re.search(r'(\d{4})å¹´(\d{1,2})æœˆ(\d{1,2})æ—¥', date_str)
            if chinese_match:
                year = chinese_match.group(1)
                month = chinese_match.group(2).zfill(2)
                day = chinese_match.group(3).zfill(2)
                return f"{year}-{month}-{day}"
            
            # æ ‡å‡†æ—¥æœŸæ ¼å¼
            standard_match = re.search(r'(\d{4})-(\d{1,2})-(\d{1,2})', date_str)
            if standard_match:
                year = standard_match.group(1)
                month = standard_match.group(2).zfill(2)
                day = standard_match.group(3).zfill(2)
                return f"{year}-{month}-{day}"
            
            return None
        except Exception as e:
            logger.error(f"è§£ææ—¥æœŸå¤±è´¥: {date_str}, é”™è¯¯: {e}")
            return None
    
    def _estimate_date_from_issue(self, issue: str) -> str:
        """ä»æœŸå·ä¼°ç®—æ—¥æœŸ"""
        try:
            year_part = int(issue[:2])
            issue_num = int(issue[2:])
            
            if year_part <= 99:
                full_year = 2000 + year_part if year_part >= 0 else 1900 + year_part
            else:
                full_year = year_part
            
            # ç®€å•ä¼°ç®—ï¼šæ¯å¹´1æœˆ1æ—¥å¼€å§‹ï¼Œæ¯å‘¨3æœŸï¼ˆäºŒã€å››ã€æ—¥ï¼‰
            weeks = (issue_num - 1) // 3
            days = weeks * 7
            
            # æ¯å‘¨çš„å“ªä¸€å¤©ï¼ˆ0=å‘¨äºŒ, 2=å‘¨å››, 5=å‘¨æ—¥ï¼‰
            day_of_week = (issue_num - 1) % 3
            day_offsets = {0: 0, 1: 2, 2: 5}
            days += day_offsets[day_of_week]
            
            # æ‰¾åˆ°ç¬¬ä¸€ä¸ªå¼€å¥–æ—¥ï¼ˆ2003å¹´ç¬¬ä¸€æœŸçš„å®é™…æ—¥æœŸæ˜¯2003-02-23ï¼‰
            if full_year == 2003:
                base_date = datetime(2003, 2, 23)
            else:
                base_date = datetime(full_year, 1, 1)
                
                # æ‰¾åˆ°ç¬¬ä¸€ä¸ªå‘¨äºŒ
                while base_date.weekday() != 1:  # å‘¨äºŒ
                    base_date += timedelta(days=1)
            
            estimated_date = base_date + timedelta(days=days)
            return estimated_date.strftime('%Y-%m-%d')
            
        except Exception as e:
            logger.error(f"ä¼°ç®—æ—¥æœŸå¤±è´¥: {issue}, é”™è¯¯: {e}")
            # è¿”å›é»˜è®¤æ—¥æœŸ
            year_part = int(issue[:2])
            full_year = 2000 + year_part if year_part >= 0 else 1900 + year_part
            return f"{full_year}-01-01"
    
    def generate_all_issues(self) -> List[str]:
        """ç”Ÿæˆæ‰€æœ‰æœŸå·"""
        issues = []
        
        # 1. è·å–æœ€æ–°æœŸå·
        latest_info = self.get_latest_issue_info()
        latest_issue = latest_info['issue']
        
        if latest_issue.startswith('26'):
            latest_num = int(latest_issue[2:])
            # ç”Ÿæˆ2026å¹´æœŸå·
            for i in range(1, latest_num + 1):
                issues.append(f"26{i:03d}")
        
        # 2. ç”Ÿæˆ2003-2025å¹´æœŸå·
        for year in range(2003, 2026):
            short_year = year % 100
            max_issue = self.year_issues.get(year, 0)
            
            if max_issue > 0:
                for issue in range(1, max_issue + 1):
                    issues.append(f"{short_year:02d}{issue:03d}")
        
        # æ’åº
        issues.sort()
        logger.info(f"ç”ŸæˆæœŸå·æ€»æ•°: {len(issues)}")
        return issues
    
    # ================ æ–°å¢æ•°æ®è·å–é€‰é¡¹ ================
    
    def crawl_single_year(self, year: int) -> List[DoubleBallRecord]:
        """çˆ¬å–å•ä¸ªå¹´ä»½æ•°æ®"""
        logger.info(f"å¼€å§‹ä¸‹è½½ {year} å¹´æ•°æ®...")
        
        # ç”ŸæˆæŒ‡å®šå¹´ä»½çš„æœŸå·
        all_issues = []
        short_year = year % 100
        max_issue = self.year_issues.get(year, 0)
        
        if max_issue > 0:
            for issue in range(1, max_issue + 1):
                all_issues.append(f"{short_year:02d}{issue:03d}")
        
        return self._crawl_issues_list(all_issues, f"{year}å¹´æ•°æ®")
    
    def crawl_recent_years(self, years: int = 3) -> List[DoubleBallRecord]:
        """çˆ¬å–æœ€è¿‘Nå¹´æ•°æ®"""
        current_year = datetime.now().year
        start_year = current_year - years + 1
        
        logger.info(f"å¼€å§‹ä¸‹è½½æœ€è¿‘ {years} å¹´æ•°æ® ({start_year}-{current_year})...")
        
        # ç”ŸæˆæŒ‡å®šå¹´ä»½èŒƒå›´çš„æœŸå·
        all_issues = []
        for year in range(start_year, current_year + 1):
            short_year = year % 100
            max_issue = self.year_issues.get(year, 0)
            
            if max_issue > 0:
                for issue in range(1, max_issue + 1):
                    all_issues.append(f"{short_year:02d}{issue:03d}")
        
        return self._crawl_issues_list(all_issues, f"æœ€è¿‘{years}å¹´æ•°æ®")
    
    def crawl_issue_range(self, start_issue: str, end_issue: str) -> List[DoubleBallRecord]:
        """çˆ¬å–æŒ‡å®šæœŸæ•°èŒƒå›´æ•°æ®"""
        logger.info(f"å¼€å§‹ä¸‹è½½ {start_issue} åˆ° {end_issue} çš„æ•°æ®...")

        try:
            # è§£ææœŸå· - æ­£ç¡®çš„5ä½æœŸå·è§£æ
            if len(start_issue) != 5 or len(end_issue) != 5:
                logger.error("æœŸå·æ ¼å¼é”™è¯¯ï¼Œåº”ä¸º5ä½æ•°å­—ï¼ˆå¦‚26001ï¼‰")
                return []

            start_year_part = int(start_issue[:2])
            start_num = int(start_issue[2:])
            end_year_part = int(end_issue[:2])
            end_num = int(end_issue[2:])

            # æ£€æŸ¥æœŸå·æ ¼å¼
            if start_year_part < 3 or end_year_part < 3:
                logger.error("å¹´ä»½éƒ¨åˆ†ä¸èƒ½å°äº03ï¼ˆ2003å¹´ï¼‰")
                return []

            if start_num < 1 or end_num < 1:
                logger.error("æœŸæ•°éƒ¨åˆ†ä¸èƒ½å°äº1")
                return []

            # å¦‚æœèµ·å§‹æœŸå·å¤§äºç»“æŸæœŸå·ï¼Œäº¤æ¢
            if start_issue > end_issue:
                logger.warning(f"èµ·å§‹æœŸå·å¤§äºç»“æŸæœŸå·ï¼Œå·²è‡ªåŠ¨äº¤æ¢: {start_issue}-{end_issue}")
                start_issue, end_issue = end_issue, start_issue
                start_year_part, end_year_part = end_year_part, start_year_part
                start_num, end_num = end_num, start_num

            # ç”ŸæˆæœŸå·èŒƒå›´
            all_issues = []

            if start_year_part == end_year_part:
                # åŒä¸€å¹´ä»½
                for issue_num in range(start_num, end_num + 1):
                    all_issues.append(f"{start_year_part:02d}{issue_num:03d}")
            else:
                # ç®€å•å¤„ç†è·¨å¹´ä»½ï¼šæŒ‰é¡ºåºæ·»åŠ æ‰€æœ‰æœŸå·
                # æ³¨æ„ï¼šè¿™é‡Œå‡è®¾å¹´ä»½æ˜¯è¿ç»­çš„ï¼Œä¸”æ¯å¹´æœ€å¤š154æœŸ
                for year_part in range(start_year_part, end_year_part + 1):
                    if year_part == start_year_part:
                        # èµ·å§‹å¹´ä»½ï¼Œä»start_numå¼€å§‹
                        issue_start = start_num
                        # è·å–è¯¥å¹´ä»½çš„æœ€å¤§æœŸæ•°
                        full_year = 2000 + year_part if year_part > 23 else 1900 + year_part
                        max_issue = self.year_issues.get(full_year, 154)
                        issue_end = max_issue
                    elif year_part == end_year_part:
                        # ç»“æŸå¹´ä»½ï¼Œåˆ°end_numç»“æŸ
                        issue_start = 1
                        issue_end = end_num
                    else:
                        # ä¸­é—´å¹´ä»½ï¼Œä»1åˆ°è¯¥å¹´ä»½çš„æœ€å¤§æœŸæ•°
                        full_year = 2000 + year_part if year_part > 23 else 1900 + year_part
                        max_issue = self.year_issues.get(full_year, 154)
                        issue_start = 1
                        issue_end = max_issue

                    # ç”Ÿæˆè¯¥å¹´ä»½çš„æœŸå·
                    for issue_num in range(issue_start, issue_end + 1):
                        all_issues.append(f"{year_part:02d}{issue_num:03d}")

            logger.info(f"ç”ŸæˆæœŸå·èŒƒå›´: å…±{len(all_issues)}æœŸ, ä»{all_issues[0]}åˆ°{all_issues[-1]}")
            return self._crawl_issues_list(all_issues, f"{start_issue}-{end_issue}æœŸæ•°æ®")

        except ValueError as e:
            logger.error(f"æœŸå·è§£æé”™è¯¯: {e}")
            return []
        except Exception as e:
            logger.error(f"ç”ŸæˆæœŸå·èŒƒå›´å¤±è´¥: {e}")
            return []

    def crawl_historical_data(self, start_year: int = 2003, end_year: int = 2025) -> List[DoubleBallRecord]:
        """çˆ¬å–å†å²æ•°æ® - å…¼å®¹åŸæœ‰æ¥å£"""
        logger.info(f"å¼€å§‹ä¸‹è½½ {start_year} åˆ° {end_year} å¹´çš„å†å²æ•°æ®...")
        
        # ç”ŸæˆæŒ‡å®šå¹´ä»½çš„æœŸå·
        all_issues = []
        for year in range(start_year, end_year + 1):
            short_year = year % 100
            max_issue = self.year_issues.get(year, 0)
            
            if max_issue > 0:
                for issue in range(1, max_issue + 1):
                    all_issues.append(f"{short_year:02d}{issue:03d}")
        
        return self._crawl_issues_list(all_issues, "å†å²æ•°æ®")
    
    def crawl_current_year_data(self, year: int = 2026) -> List[DoubleBallRecord]:
        """çˆ¬å–å½“å‰å¹´ä»½æ•°æ® - å…¼å®¹åŸæœ‰æ¥å£"""
        logger.info(f"å¼€å§‹ä¸‹è½½ {year} å¹´æœ€æ–°æ•°æ®...")
        
        # è·å–æœ€æ–°æœŸå·ä¿¡æ¯
        latest_info = self.get_latest_issue_info()
        latest_issue = latest_info['issue']
        
        # ç”Ÿæˆå½“å‰å¹´æœŸå·
        issues = []
        if latest_issue.startswith(str(year % 100)):
            latest_num = int(latest_issue[2:])
            short_year = year % 100
            for i in range(1, latest_num + 1):
                issues.append(f"{short_year:02d}{i:03d}")
        
        return self._crawl_issues_list(issues, f"{year}å¹´æ•°æ®")
    
    def _crawl_issues_list(self, issues: List[str], description: str) -> List[DoubleBallRecord]:
        """çˆ¬å–æŒ‡å®šçš„æœŸå·åˆ—è¡¨"""
        records = []
        total = len(issues)
        
        if total == 0:
            logger.warning(f"æ²¡æœ‰éœ€è¦çˆ¬å–çš„{description}")
            return records
        
        logger.info(f"éœ€è¦çˆ¬å– {total} æœŸ{description}")
        
        success = 0
        skip = 0
        fail = 0
        
        for i, issue in enumerate(issues, 1):
            # æ˜¾ç¤ºè¿›åº¦
            if i % 10 == 0 or i <= 5 or i >= total - 5:
                progress = i / total * 100
                logger.info(f"è¿›åº¦: {progress:.1f}% ({i}/{total}) - æœŸå·: {issue}")
            
            # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
            existing = self.db.get_record_by_issue(issue)
            if existing:
                skip += 1
                continue
            
            # çˆ¬å–æ•°æ®
            record = self.crawl_single_period(issue)
            if record and record.is_valid():
                records.append(record)
                success += 1
                
                # æ¯10æ¡ä¿å­˜ä¸€æ¬¡
                if success % 10 == 0:
                    self.db.save_records(records[-10:])
            else:
                fail += 1
                logger.warning(f"çˆ¬å–å¤±è´¥: {issue}")
            
            # æ§åˆ¶è¯·æ±‚é¢‘ç‡
            if i % 50 == 0:
                rest_time = random.uniform(10, 20)
                logger.info(f"å·²å¤„ç† {i} æœŸï¼Œä¼‘æ¯ {rest_time:.1f} ç§’...")
                time.sleep(rest_time)
            else:
                time.sleep(random.uniform(1, 3))
        
        # ä¿å­˜å‰©ä½™è®°å½•
        if records:
            self.db.save_records(records)
        
        # æ˜¾ç¤ºç»“æœ
        logger.info(f"{description}çˆ¬å–å®Œæˆ!")
        logger.info(f"æˆåŠŸ: {success} æœŸ, è·³è¿‡: {skip} æœŸ, å¤±è´¥: {fail} æœŸ")
        
        return records
    
    def sync_all_data_incremental(self, force_update: bool = False) -> Dict[str, Any]:
        """åŒæ­¥æ‰€æœ‰æ•°æ® - å¢é‡ç‰ˆæœ¬ï¼ˆå…¼å®¹åŸæœ‰æ¥å£ï¼‰"""
        if force_update:
            self.db.clear_all_data()
            logger.info("å·²æ¸…ç©ºæ‰€æœ‰æ•°æ®ï¼Œå¼€å§‹é‡æ–°åŒæ­¥")
        
        # è·å–æœ€æ–°æœŸå·ä¿¡æ¯
        latest_info = self.get_latest_issue_info()
        
        # çˆ¬å–å†å²æ•°æ®
        historical_records = self.crawl_historical_data(2003, 2025)
        
        # çˆ¬å–å½“å‰å¹´ä»½æ•°æ®
        current_records = self.crawl_current_year_data(2026)
        
        total_records = len(historical_records) + len(current_records)
        
        return {
            'historical_records': len(historical_records),
            'current_records': len(current_records),
            'total_records': total_records,
            'latest_issue': latest_info.get('issue', 'æœªçŸ¥'),
            'incremental': True
        }
    
    def test_connection(self) -> bool:
        """æµ‹è¯•è¿æ¥"""
        try:
            test_url = f"{self.base_url}/26001.html"
            response = self.smart_request(test_url)
            
            if response and response.status_code == 200:
                logger.info("âœ… è¿æ¥æµ‹è¯•æˆåŠŸ")
                return True
            else:
                logger.warning("è¿æ¥æµ‹è¯•å¤±è´¥")
                return False
        except Exception as e:
            logger.error(f"è¿æ¥æµ‹è¯•å¼‚å¸¸: {e}")
            return False
    
    def get_database_stats(self) -> Dict[str, Any]:
        """è·å–æ•°æ®åº“ç»Ÿè®¡ä¿¡æ¯"""
        try:
            stats = self.db.get_database_info()
            record_count = self.db.get_record_count()
            years_with_data = self.db.get_years_with_data()
            issue_range = self.db.get_issue_range()
            date_range = self.db.get_date_range()
            
            return {
                'record_count': record_count,
                'years_with_data': years_with_data,
                'issue_range': issue_range,
                'date_range': date_range,
                'database_path': stats.get('database_path', 'æœªçŸ¥'),
                'database_size': stats.get('database_size', 'æœªçŸ¥')
            }
        except Exception as e:
            logger.error(f"è·å–æ•°æ®åº“ç»Ÿè®¡å¤±è´¥: {e}")
            return {}
    
    def cleanup(self):
        """æ¸…ç†èµ„æº"""
        pass  # è¿™ä¸ªç‰ˆæœ¬ä¸éœ€è¦ç‰¹æ®Šçš„æ¸…ç†
    
    def __del__(self):
        """ææ„å‡½æ•°"""
        self.cleanup()

# ================ å¢å¼ºçš„ä¸»ç¨‹åºèœå• ================

def display_menu():
    """æ˜¾ç¤ºä¸»èœå•"""
    print("\n" + "="*70)
    print("ğŸ¯ åŒè‰²çƒæ•°æ®é‡‡é›†ç³»ç»Ÿ - å¢å¼ºç‰ˆ")
    print("="*70)
    print("1.  ğŸš€ åŒæ­¥æ‰€æœ‰æ•°æ®ï¼ˆå†å²+å½“å‰ï¼‰")
    print("2.  ğŸ“Š æŸ¥çœ‹æ•°æ®åº“ç»Ÿè®¡")
    print("3.  ğŸ”„ æµ‹è¯•è¿æ¥å’Œæœ€æ–°æœŸå·")
    print("4.  ğŸ“… æµ‹è¯•å•ä¸ªå¹´ä»½ (2023å¹´)")
    print("5.  ğŸ“ˆ è·å–æœ€è¿‘3å¹´æ•°æ® (2023-2025)")
    print("6.  ğŸ—“ï¸  è·å–æŒ‡å®šå¹´ä»½æ•°æ®")
    print("7.  ğŸ”¢ è·å–æŒ‡å®šæœŸæ•°èŒƒå›´ï¼ˆå¦‚26002è‡³26017ï¼‰")
    print("8.  ğŸ“š è·å–å…¨éƒ¨å†å²æ•°æ® (2003-2025)")
    print("9.  ğŸ†• è·å–å½“å‰å¹´ä»½æ•°æ® (2026)")
    print("10. ğŸ§¹ æ¸…ç©ºæ•°æ®åº“ï¼ˆè°¨æ…ä½¿ç”¨ï¼ï¼‰")
    print("11. ğŸšª é€€å‡º")
    print("="*70)

def get_year_input(prompt: str) -> int:
    """è·å–å¹´ä»½è¾“å…¥"""
    while True:
        try:
            year = int(input(prompt))
            if 2003 <= year <= datetime.now().year:
                return year
            else:
                print(f"âŒ å¹´ä»½å¿…é¡»åœ¨2003-{datetime.now().year}ä¹‹é—´")
        except ValueError:
            print("âŒ è¯·è¾“å…¥æœ‰æ•ˆçš„å¹´ä»½æ•°å­—")

def get_issue_input(prompt: str) -> str:
    """è·å–æœŸå·è¾“å…¥"""
    while True:
        issue = input(prompt).strip()
        if len(issue) == 5 and issue.isdigit():
            # æ£€æŸ¥æœŸå·æ ¼å¼
            year_part = int(issue[:2])
            issue_num = int(issue[2:])
            # å¹´ä»½éƒ¨åˆ†ä»03å¼€å§‹ï¼ˆ2003å¹´ï¼‰ï¼ŒæœŸæ•°ä»1å¼€å§‹
            if 3 <= year_part <= 99 and 1 <= issue_num <= 154:
            # if year_part >= 3 and issue_num >= 1:
                return issue
        print("âŒ è¯·è¾“å…¥æœ‰æ•ˆçš„5ä½æœŸå·ï¼ˆå¦‚26001ï¼‰")

# å…¼å®¹åŸæœ‰ä»£ç çš„ä¸»å‡½æ•°
if __name__ == "__main__":
    """å¢å¼ºç‰ˆä¸»ç¨‹åº"""
    import sys
    import os
    
    # æ·»åŠ è·¯å¾„
    sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
    
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    db = DoubleBallDatabase()
    crawler = DoubleBallCrawler(db)
    
    try:
        # æ˜¾ç¤ºæ¬¢è¿ä¿¡æ¯
        print("\n" + "="*70)
        print("ğŸ¯ åŒè‰²çƒæ•°æ®é‡‡é›†ç³»ç»Ÿ - å¢å¼ºç‰ˆ")
        print("="*70)
        
        while True:
            display_menu()
            
            try:
                choice = input("\nè¯·é€‰æ‹©é€‰é¡¹ (1-11): ").strip()
                
                if choice == '1':
                    # ğŸš€ åŒæ­¥æ‰€æœ‰æ•°æ®
                    print("\nå¼€å§‹åŒæ­¥æ‰€æœ‰æ•°æ®...")
                    result = crawler.sync_all_data_incremental()
                    print(f"åŒæ­¥å®Œæˆ: {result}")
                    
                elif choice == '2':
                    # ğŸ“Š æŸ¥çœ‹æ•°æ®åº“ç»Ÿè®¡
                    stats = crawler.get_database_stats()
                    print(f"\nğŸ“Š æ•°æ®åº“ç»Ÿè®¡:")
                    print(f"  è®°å½•æ€»æ•°: {stats.get('record_count', 0)} æœŸ")
                    print(f"  æœŸå·èŒƒå›´: {stats.get('issue_range', {}).get('min_issue', 'æœªçŸ¥')} - {stats.get('issue_range', {}).get('max_issue', 'æœªçŸ¥')}")
                    print(f"  æ—¥æœŸèŒƒå›´: {stats.get('date_range', {}).get('min_date', 'æœªçŸ¥')} - {stats.get('date_range', {}).get('max_date', 'æœªçŸ¥')}")
                    print(f"  æ•°æ®å¹´ä»½: {stats.get('years_with_data', [])}")
                    
                elif choice == '3':
                    # ğŸ”„ æµ‹è¯•è¿æ¥å’Œæœ€æ–°æœŸå·
                    print("\næµ‹è¯•è¿æ¥å’Œæœ€æ–°æœŸå·...")
                    if crawler.test_connection():
                        latest_info = crawler.get_latest_issue_info()
                        print(f"âœ… è¿æ¥æ­£å¸¸")
                        print(f"ğŸ“… æœ€æ–°æœŸå·: {latest_info['issue']}")
                        print(f"ğŸ“… æ¥æº: {latest_info['source']}")
                    else:
                        print("âŒ è¿æ¥å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç½‘ç»œ")
                        
                elif choice == '4':
                    # ğŸ“… æµ‹è¯•å•ä¸ªå¹´ä»½ (2023å¹´)
                    print("\nå¼€å§‹ä¸‹è½½2023å¹´æ•°æ®...")
                    records = crawler.crawl_single_year(2023)
                    print(f"âœ… 2023å¹´æ•°æ®ä¸‹è½½å®Œæˆï¼Œå…± {len(records)} æœŸ")
                    
                elif choice == '5':
                    # ğŸ“ˆ è·å–æœ€è¿‘3å¹´æ•°æ®
                    print("\nå¼€å§‹ä¸‹è½½æœ€è¿‘3å¹´æ•°æ® (2023-2025)...")
                    records = crawler.crawl_recent_years(3)
                    print(f"âœ… æœ€è¿‘3å¹´æ•°æ®ä¸‹è½½å®Œæˆï¼Œå…± {len(records)} æœŸ")
                    
                elif choice == '6':
                    # ğŸ—“ï¸ è·å–æŒ‡å®šå¹´ä»½æ•°æ®
                    year = get_year_input("\nè¯·è¾“å…¥è¦ä¸‹è½½çš„å¹´ä»½ (2003-2026): ")
                    print(f"\nå¼€å§‹ä¸‹è½½{year}å¹´æ•°æ®...")
                    records = crawler.crawl_single_year(year)
                    print(f"âœ… {year}å¹´æ•°æ®ä¸‹è½½å®Œæˆï¼Œå…± {len(records)} æœŸ")
                    
                elif choice == '7':
                    # ğŸ”¢ è·å–æŒ‡å®šæœŸæ•°èŒƒå›´
                    print("\nè·å–æŒ‡å®šæœŸæ•°èŒƒå›´æ•°æ®")
                    start_issue = get_issue_input("è¯·è¾“å…¥èµ·å§‹æœŸå· (å¦‚26002): ")
                    end_issue = get_issue_input("è¯·è¾“å…¥ç»“æŸæœŸå· (å¦‚26017: ")
                    
                    if start_issue > end_issue:
                        print("âŒ èµ·å§‹æœŸå·ä¸èƒ½å¤§äºç»“æŸæœŸå·")
                        continue
                    
                    print(f"\nå¼€å§‹ä¸‹è½½ {start_issue} åˆ° {end_issue} çš„æ•°æ®...")
                    records = crawler.crawl_issue_range(start_issue, end_issue)
                    print(f"âœ… æŒ‡å®šæœŸæ•°èŒƒå›´æ•°æ®ä¸‹è½½å®Œæˆï¼Œå…± {len(records)} æœŸ")
                    
                elif choice == '8':
                    # ğŸ“š è·å–å…¨éƒ¨å†å²æ•°æ®
                    print("\nå¼€å§‹ä¸‹è½½å…¨éƒ¨å†å²æ•°æ® (2003-2025)...")
                    records = crawler.crawl_historical_data(2003, 2025)
                    print(f"âœ… å…¨éƒ¨å†å²æ•°æ®ä¸‹è½½å®Œæˆï¼Œå…± {len(records)} æœŸ")
                    
                elif choice == '9':
                    # ğŸ†• è·å–å½“å‰å¹´ä»½æ•°æ®
                    current_year = datetime.now().year
                    print(f"\nå¼€å§‹ä¸‹è½½{current_year}å¹´æ•°æ®...")
                    records = crawler.crawl_current_year_data(current_year)
                    print(f"âœ… {current_year}å¹´æ•°æ®ä¸‹è½½å®Œæˆï¼Œå…± {len(records)} æœŸ")
                    
                elif choice == '10':
                    # ğŸ§¹ æ¸…ç©ºæ•°æ®åº“
                    confirm = input("\nâš ï¸  è­¦å‘Šï¼šæ­¤æ“ä½œå°†æ¸…ç©ºæ‰€æœ‰æ•°æ®ï¼ç¡®å®šç»§ç»­å—ï¼Ÿ(y/N): ").strip().lower()
                    if confirm == 'y' or confirm == 'yes':
                        db.clear_all_data()
                        print("âœ… æ•°æ®åº“å·²æ¸…ç©º")
                    else:
                        print("âŒ æ“ä½œå·²å–æ¶ˆ")
                        
                elif choice == '11':
                    # ğŸšª é€€å‡º
                    print("\nğŸ‘‹ å†è§ï¼")
                    break
                    
                else:
                    print("âŒ æ— æ•ˆé€‰é¡¹ï¼Œè¯·é‡æ–°é€‰æ‹©")
                    
            except KeyboardInterrupt:
                print("\n\nâš ï¸  ç”¨æˆ·ä¸­æ–­æ“ä½œ")
                break
            except Exception as e:
                print(f"âŒ æ“ä½œå¤±è´¥: {e}")
                import traceback
                traceback.print_exc()
                
    except Exception as e:
        print(f"âŒ ç¨‹åºå¯åŠ¨å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
    finally:
        crawler.cleanup()
# [file content end]
